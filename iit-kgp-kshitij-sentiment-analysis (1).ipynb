{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next few code blocks would just be repetition of a few steps from the original notebook,<br>\n",
    "Please skip to continue from the new additions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:03:04.557539Z",
     "iopub.status.busy": "2025-02-07T18:03:04.557293Z",
     "iopub.status.idle": "2025-02-07T18:03:33.582085Z",
     "shell.execute_reply": "2025-02-07T18:03:33.581407Z",
     "shell.execute_reply.started": "2025-02-07T18:03:04.557518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e5ba9df18e4cc9b042ba5b5561c1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58973f1b92045f69d21a1921b937f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7a983a46e7466da4733af97453f5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2a8645f5a1410ea53d323bc5c157e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585584c5ab384a9a8d80eafc4fef2645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"/kaggle/input/aihsentiment/AI-Hackathon-test-data-set-3.csv\")\n",
    "pos_data = data[data[\"Sentiment\"] == 1]\n",
    "neg_data = data[data[\"Sentiment\"] == -1]\n",
    "net_data = data[data[\"Sentiment\"] == 0]\n",
    "use_pos = pos_data[:4000]\n",
    "use_neg = neg_data[:4000]\n",
    "data1 = pd.concat([use_pos, use_neg, net_data])\n",
    "from sklearn.utils import shuffle\n",
    "data1 = shuffle(data1, random_state = 42)\n",
    "x = data1[\"Comments\"]\n",
    "y = data1[\"Sentiment\"]\n",
    "\n",
    "x_train = x[:int(len(x)*0.75)]\n",
    "y_train = y[:int(len(y)*0.75)]\n",
    "x_test = x[int(len(x)*0.75):]\n",
    "y_test = y[int(len(y)*0.75):]\n",
    "from transformers import BertTokenizer as BT, TFBertForSequenceClassification as tbsc\n",
    "from sklearn.metrics import classification_report as cr\n",
    "tokenizer = BT.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "x_train_en = tokenizer.batch_encode_plus(x_train.astype(str).tolist(),\n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = 128,\n",
    "                                              return_tensors='tf')\n",
    " \n",
    "x_test_en= tokenizer.batch_encode_plus(x_test.astype(str).tolist(),\n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = 128,\n",
    "                                              return_tensors='tf')\n",
    "model = tbsc.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "y_train = y_train + 1\n",
    "y_test = y_test + 1\n",
    "import tensorflow as tf\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is where the continuation of the notebook starts from**<br><br>\n",
    "We start with the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:03:33.583149Z",
     "iopub.status.busy": "2025-02-07T18:03:33.582865Z",
     "iopub.status.idle": "2025-02-07T18:05:41.212262Z",
     "shell.execute_reply": "2025-02-07T18:05:41.211540Z",
     "shell.execute_reply.started": "2025-02-07T18:03:33.583126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "286/286 [==============================] - 70s 109ms/step - loss: 0.2305 - accuracy: 0.9387 - val_loss: 0.1629 - val_accuracy: 0.9524\n",
      "Epoch 2/3\n",
      "286/286 [==============================] - 28s 99ms/step - loss: 0.1464 - accuracy: 0.9594 - val_loss: 0.1573 - val_accuracy: 0.9573\n",
      "Epoch 3/3\n",
      "286/286 [==============================] - 29s 102ms/step - loss: 0.1458 - accuracy: 0.9578 - val_loss: 0.1622 - val_accuracy: 0.9524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7915b8aa3e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [x_train_en['input_ids'], x_train_en['token_type_ids'], x_train_en['attention_mask']],\n",
    "    y_train,\n",
    "    validation_data=(\n",
    "      [x_test_en['input_ids'], x_test_en['token_type_ids'], x_test_en['attention_mask']],y_test),\n",
    "    batch_size=32,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the evaluation shows, our model gains an accuracy of about ~96%.<br>\n",
    "Lets evaluate our model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:05:41.213508Z",
     "iopub.status.busy": "2025-02-07T18:05:41.213205Z",
     "iopub.status.idle": "2025-02-07T18:05:44.386145Z",
     "shell.execute_reply": "2025-02-07T18:05:44.385521Z",
     "shell.execute_reply.started": "2025-02-07T18:05:41.213484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 3s 32ms/step - loss: 0.1622 - accuracy: 0.9524\n",
      "Test loss: 0.1621810644865036, Test accuracy: 0.9523965716362\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(\n",
    "    [x_test_en['input_ids'], x_test_en['token_type_ids'], x_test_en['attention_mask']],\n",
    "    y_test\n",
    ")\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model performs well on the test data as well.<br>\n",
    "We can proceed with the use of this model for our goal.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: User interactive space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we allow the model to be used for different data.<br>\n",
    "\n",
    "**Note**: The performance of this model on real world data is extremely poor,<br>\n",
    "due to the source dataset having effectively only 22 data points.<br>\n",
    "As the motive of the hackathon's organizers was to test the preprocessing abilities of the participants, this model's performance is relevant in the scope of evaluating against the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:05:44.387484Z",
     "iopub.status.busy": "2025-02-07T18:05:44.387117Z",
     "iopub.status.idle": "2025-02-07T18:06:19.204157Z",
     "shell.execute_reply": "2025-02-07T18:06:19.203251Z",
     "shell.execute_reply.started": "2025-02-07T18:05:44.387448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the array of reviews you want to predict.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Good product\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/286 [==============================] - 12s 33ms/step\n",
      "Predicted sentiments are - \n",
      "\n",
      "Good product : Positive\n",
      "Do you wish to continue?\n",
      "1 - Yes\n",
      "2 - No\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"Enter the array of reviews you want to predict.\")\n",
    "    arr = list(input().rstrip().split(\", \"))\n",
    "    df = pd.DataFrame(arr, columns=['Comments'])\n",
    "    df_en = tokenizer.batch_encode_plus(x_train.astype(str).tolist(),\n",
    "                                        padding=True, \n",
    "                                        truncation=True,\n",
    "                                        max_length = 128,\n",
    "                                        return_tensors='tf')\n",
    "    pred = model.predict(\n",
    "        [df_en['input_ids'],df_en['token_type_ids'], df_en['attention_mask']])\n",
    "    logits = pred.logits\n",
    "    pred_labels = tf.argmax(logits, axis=1)\n",
    "    pred_labels = pred_labels.numpy()\n",
    "    label = {\n",
    "        2: 'Positive',\n",
    "        0: 'Negative',\n",
    "        1: 'Neutral'\n",
    "    }\n",
    "    pred_labels = [label[i] for i in pred_labels]\n",
    "    print(\"Predicted sentiments are - \\n\")\n",
    "    for i in range(len(arr)):\n",
    "        print(arr[i],\":\",pred_labels[i])\n",
    "    print(\"Do you wish to continue?\\n1 - Yes\\n2 - No\")\n",
    "    if int(input()) == 2:\n",
    "        break\n",
    "\n",
    "print(\"Thank you.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This marks the end of our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is aimed at determining the sentiment of reviews of products accurately.<br>\n",
    "The true core challenge of the project was to counter the challenges faced by having a poor dataset.<br>\n",
    "Challenges faces in the dataset -\n",
    "- Heavy redundancy (>98%), causing no real input value in the dataset (the dataset had 22 rows post dropping duplicate values, which originally had 20000 rows)\n",
    "- Wrong, with low variety in data in the first place, different instances of identical features having different label added to the loss of value in the data<br>\n",
    "\n",
    "Due to the above reasons (very low variety in data), the model trained on the same is essentially not of practical use.<br><br>\n",
    "With the challenge being to make the model accurate for data within the dataset itself,<br>\n",
    "our model gained an accuracy of ~96%, which originally hit an bottleneck accuracy of 60% prior to all preprocessing ideas being implemented.\n",
    "With that, the goal has been met & the project is complete.\n",
    "\n",
    "**What makes this project different (& better)?**\n",
    "-> Accuracy of ~95% compared to the bottleneck average of 60%<br><br>\n",
    "What was the main factor behind the accuracy increase?<br>\n",
    "Resolving massive data annotation errors in the original dataset.<br><br>\n",
    "Due to the inconsistent annotation, fitting the model with the entire dataset couldn't reproduce a high accuracy. The reason being the same data being present multiple times, with different annotations."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6504742,
     "sourceId": 10507075,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6507633,
     "sourceId": 10513307,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
